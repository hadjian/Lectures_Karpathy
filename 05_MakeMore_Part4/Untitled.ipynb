{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2673859b-4d8a-4881-8fa3-11860cdb9878",
   "metadata": {},
   "source": [
    "# Becoming a Backprop Ninja\n",
    "* You need to understand backpropagation, because it is a leaky abstraction\n",
    "* We already covered backpropagation for the scalar case, by implementing micrograd\n",
    "* But we need to expand this knowledge to tensors\n",
    "* We will use the same neural network as in the last lecture, but this time we will implement the backward pass manually"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78c9a6e-1466-4519-965d-f6a239dc480a",
   "metadata": {},
   "source": [
    "## The Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c92b2632-e797-4c34-9a8d-86a0a01468ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfa6c142-c68f-4995-aecf-ea2824a66197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n",
      "15\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
     ]
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "print(len(words))\n",
    "print(max(len(w) for w in words))\n",
    "print(words[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09cdcbfc-6da8-4460-b353-8b43331b4f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = list(sorted(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "vocab_size = len(stoi)\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "75291b4a-8782-4c1e-a0ba-e6596ea2c272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dataset\n",
    "block_size = 3\n",
    "def build_dataset(words):\n",
    "    X,Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * 3\n",
    "        for c in w+'.':\n",
    "            xi = stoi[c]\n",
    "            X.append(context)\n",
    "            Y.append(xi)\n",
    "            context = context[1:]+[xi]\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X,Y\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "Xtr,Ytr=build_dataset(words[0:n1])\n",
    "Xval,Yval=build_dataset(words[n1:n2])\n",
    "Xtest, Ytest=build_dataset(words[n2:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef86fbd-2aa0-4708-b27b-ba5e776d2f23",
   "metadata": {},
   "source": [
    "* A new utility function is introduced that compares our manual gradient computations with pytorch computed ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "895ee718-104d-42dd-9e12-02731e89480c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmp(s, dt, t):\n",
    "    ex = torch.all(dt == t.grad).item()\n",
    "    app = torch.allclose(dt, t.grad)\n",
    "    maxdiff = (dt - t.grad).abs().max()\n",
    "    print(f'{s:15s} | exact {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4de40b8-5106-4928-89fb-432186628c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10\n",
    "n_hidden = 64\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size, n_embd)               , generator = g)\n",
    "W1 = torch.randn((block_size*n_embd, n_hidden)     , generator = g) * 5/3 / ((block_size*n_embd)**0.5) # kaimin initialization to avoid contraction\n",
    "b1 = torch.randn((n_hidden)                        , generator = g) * 0.1\n",
    "W2 = torch.randn((n_hidden, vocab_size)            , generator = g) * 0.1  # make less confident\n",
    "b2 = torch.randn((vocab_size)                      , generator = g) * 0.1  # not zero to unmask gradient errors\n",
    "\n",
    "bngain = torch.ones((1, n_hidden)) * 0.1 + 1.0\n",
    "bnbias = torch.zeros((1, n_hidden)) * 0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd31673-2a92-4953-87d9-0acb479d650e",
   "metadata": {},
   "source": [
    "* We will do a single forward pass and for this calculate one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a347a472-e6c2-4097-b57c-753009b1594c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size # shorter name for use in expressions\n",
    "xi = torch.randint(0, len(words), (batch_size, ), generator = g)\n",
    "Xb, Yb = Xtr[xi], Ytr[xi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ace2fe6a-deed-47a8-9797-bcc756edd68d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3698, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# forward pass \"chunkated\" into smaller steps that are possible to backward one at a time\n",
    "emb = C[Xb]\n",
    "embcat = emb.view(emb.shape[0], -1)\n",
    "# Linear Layer 1\n",
    "hprebn = embcat @ W1 + b1\n",
    "# Batchnorm Layer\n",
    "bnmeani = hprebn.mean(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1/(n-1) * bndiff2.sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1 instead of n)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "# Non Linearity\n",
    "h = torch.tanh(hpreact)\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2\n",
    "# cross entropy loss\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# PyTorch backward pass\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv,\n",
    "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
    "          bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
    "          embcat, emb]:\n",
    "    t.retain_grad()\n",
    "loss.backward()\n",
    "loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d09fbc-f949-4cee-b553-caeeaf95103a",
   "metadata": {},
   "source": [
    "* Exercise 1: backprop through the whole thing manually, backpropagating through exactly all of the variables as they are defined in the forward pass above, one by one\n",
    "* We start with dlogprops, which has the following shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9527d14-d38d-4f53-9c4c-50a6c1991b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprobs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3264bd3-ece9-46a5-ba77-44d0780af050",
   "metadata": {},
   "source": [
    "* The gradient tensor must have the same shape as we need the element-wise gradient\n",
    "* So how does ```logprobs``` influence ```loss```?\n",
    "* ```loss``` is a result of an index operation into ```logprobs``` and a mean calculation of all the resulting values\n",
    "* The result is then negated\n",
    "* The indices for each row of ```logprobs``` is taken from the row vector ```Yb```, which are all the correct labels for the 32 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4d29438-71c5-42a3-88d1-f37d47f5c7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5,  0,  0, 13,  8, 19, 12,  0,  0, 14,  2, 14, 15, 20, 11, 11, 22, 13,\n",
      "        15,  1, 10, 18, 20, 19,  1,  9, 10, 25,  0, 15,  5,  9])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(Yb)\n",
    "Yb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec0a41a-2c09-40b5-bf31-c3aaadad5d18",
   "metadata": {},
   "source": [
    "* So in a simpler example, where we have three indexes, the loss becomes\n",
    "  \n",
    "  $loss=-(a+b+c)/3=-a/3 - b/3 - c/3$\n",
    "* So deriving by each variable becomes for the example of $a$:\n",
    "  \n",
    "  $dloss/da=-1/3$\n",
    "\n",
    "* Or more generally $-1/n$ for $n$ variables\n",
    "* But only one number in the 32 rows is used in the $loss$ calculation, so the rest don't influence the $loss$ at all and thus receive a gradient of 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f36a46ce-3f98-4142-9467-c4a0286d073a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs[range(n), Yb] = -1.0/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "321f9ae5-f148-4614-bd25-61ccd2a7d4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "cmp('logprobs', dlogprobs, logprobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367617d7-0f14-432c-961a-38d482272f4f",
   "metadata": {},
   "source": [
    "* We continue with deriving ```logprobs``` by it's variables\n",
    "* ```logprobs``` takes as it's variable only ```probs``` and results from applying ```log``` to every entry\n",
    "* So each entry of ```probs``` changes ```logprobs``` by the proportionality factor of the derivative of ```log```, which is ```1/x```, where ```x``` is the entry\n",
    "* As ```logprobs``` and ```probs``` have the same dimensions, we just calculate ```1/x``` for each entry\n",
    "* Also we need to apply the chain rule, by multiplying by the gradients of logprobs to arrive at the derivatives of ```probs``` with respect to ```loss```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b865306d-aabd-42da-b2ed-4e53169d15c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dprobs = 1/probs * dlogprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d47ed42-2677-4186-af0d-48e06640eec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dprobs          | exact True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "cmp('dprobs', dprobs, probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc0465a-2b3c-4c60-aed5-190392c1afe3",
   "metadata": {},
   "source": [
    "* Moving on to the constituents of ```probs```, which is simply computed by multiplying the variables ```counts``` and ```counts_sum_inv```\n",
    "* Applying the derivative for multiplication (example $a$)\n",
    "  \n",
    "  $d(counts\\_sum\\_inv*counts)/dcounts\\_sum\\_inv=counts$\n",
    "* So for ```counts_sum_inv```, it will be a gradient of ```counts```\n",
    "* But we need to take into account that ```counts_sum_inv``` has a different dimensionality than ```probs```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f43f0bbc-fa4f-4d98-ad7e-5ad08570fef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]), torch.Size([32, 1]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape, counts_sum_inv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23b0b43-fa5a-4eef-b006-2c73646357ee",
   "metadata": {},
   "source": [
    "* We can see, that the one column of $counts_sum_inv$ will be broadcast into every of the 27 columns of ```probs```\n",
    "* We learned in the micrograd lecture that if a variable takes part in multiple expressions the gradients of those expression must be summed for that variable\n",
    "* One element of the ```counts_sum_inv``` column vector is used in 27 multiplications in one row of ```counts```, thus we will sum all the gradients in that row, which is the sum of ```counts```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d337564-8ee4-4d24-a846-c7f2053824fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dcounts_sum_inv | exact True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dcounts_sum_inv= (counts*dprobs).sum(1, keepdim=True)\n",
    "cmp('dcounts_sum_inv', dcounts_sum_inv, counts_sum_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4392ecc-1660-4178-bcb7-934f4c1a3304",
   "metadata": {},
   "source": [
    "* Next up is ```counts```, which appears in the two expressions that result in ```probs``` and ```counts_sum```\n",
    "* So before we can compute the derivative of ```counts```, we first need to derive ```counts_sum_inv``` wrt ```counts_sum```\n",
    "* ```counts_sum``` is inverted, so the derivative becomes\n",
    "\n",
    "  $d(counts\\_sum^{-1})/dcounts\\_sum=-counts\\_sum^{-2}$\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40da0065-7a16-487f-a56a-20ac3f3811a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dcounts_sum     | exact True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dcounts_sum = -1*counts_sum**-2 * dcounts_sum_inv\n",
    "cmp('dcounts_sum', dcounts_sum, counts_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c673a5-de8e-4cee-bcfe-eb22c411f637",
   "metadata": {},
   "source": [
    "* Now we can compute the derivative of ```counts```\n",
    "* As all variables of ```counts``` are summed row wise in ```counts_sum```, the derivative will be one for each variable\n",
    "\n",
    "  $d(counts\\_sum)/dcounts=1$\n",
    "\n",
    "* In ```probs```, ```counts``` is multiplied with the broadcasted ```counts_sum_inv```, so\n",
    "\n",
    "  $d(probs)/dcounts=d(counts * counts\\_sum\\_inv)/dcounts=counts\\_sum\\_inv$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92684425-fc1b-43a5-9c91-2da7836ea320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dcounts         | exact True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dcounts = torch.ones_like(counts) * dcounts_sum\n",
    "dcounts += counts_sum_inv * dprobs\n",
    "cmp('dcounts', dcounts, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87d054c0-a460-4522-b522-f7295ac9b306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dnorm_logits    | exact True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dnorm_logits = counts * dcounts\n",
    "cmp('dnorm_logits', dnorm_logits, norm_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3bc071e1-1b37-47f5-803c-dd8c5c2b3c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dlogit_maxes    | exact True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dlogit_maxes = (-dnorm_logits).sum(1, keepdim=True)\n",
    "cmp('dlogit_maxes', dlogit_maxes, logit_maxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08500a4a-bba0-4c5d-8286-95d11469d757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dlogits         | exact True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dlogits = torch.zeros_like(logits)\n",
    "dlogits[range(n), logits.max(1).indices] = 1\n",
    "dlogits *= dlogit_maxes\n",
    "dlogits += dnorm_logits.clone()\n",
    "cmp('dlogits', dlogits, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec4a263e-a0e3-4421-8880-6ddb29275ccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 64])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dW2 = torch.ones_like(W2)\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48eae1b6-87d1-4703-ab50-c69c966480e5",
   "metadata": {},
   "source": [
    "* Now we move on to ```logits = h @ W2 + b2```\n",
    "* Writing down the whole matrix multiply element by element shows that the partial derivatives of the expression ```h @ W2 + b2``` are also matrix multiplications, in particular\n",
    "\n",
    "  $dL/dh = dL/dlogits * W2^T$\n",
    "  \n",
    "  $dL/dW2 = h^T * dL/dlogits$\n",
    "\n",
    "* The offset b2 is broadcast across the columns of the result matrix, so each entry in a column of the $dL/dlogits$ matrix will add to the partial derivative of the $b1$ tensor\n",
    "  \n",
    "  $dL/db2 = dL/dlogits.sum(0)$\n",
    "\n",
    "* A shortcut to avoid having to remember these formulas is to look at the shapes of the operations, which have to match up\n",
    "* For example, the result of $dL/dh$ must match the dimensions of $h$\n",
    "* It results from a matrix/vector multiplication of the other factor in the product ($W2$ in this example) with $dL/dlogits$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6bf956e3-b1aa-456c-a4b1-0f6b6ea18167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: torch.Size([32, 64])\n",
      "W2: torch.Size([64, 27])\n",
      "dlogits: torch.Size([32, 27])\n"
     ]
    }
   ],
   "source": [
    "print(f'h: {h.shape}')\n",
    "print(f'W2: {W2.shape}')\n",
    "print(f'dlogits: {dlogits.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59f945e-7d99-4276-83d4-1082a6060b36",
   "metadata": {},
   "source": [
    "* The only way we arrive at a dimension of ```[32,200]``` is to multiply ```dlogits``` with ```W2``` transposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b7d03f88-e92a-43dd-8821-203fabb843a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h               | exact True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dh = dlogits @ W2.T\n",
    "cmp('h', dh, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d34931-93ef-48c2-8d07-4efd829a88f1",
   "metadata": {},
   "source": [
    "* Same for $dL/dW2$, which must have the same dimensions as ```W2``` and results from a matrix/vector multiplication of ```dlogits``` and ```h```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb088b75-75c5-433f-ac4a-8147bdb7242e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2: torch.Size([64, 27])\n",
      "h: torch.Size([32, 64])\n",
      "dlogits: torch.Size([32, 27])\n"
     ]
    }
   ],
   "source": [
    "print(f'W2: {W2.shape}')\n",
    "print(f'h: {h.shape}')\n",
    "print(f'dlogits: {dlogits.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d232b16e-2763-40d8-88fc-6b76d125fcae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW2             | exact True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dW2 = h.T @ dlogits\n",
    "cmp('dW2', dW2, W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796e19f8-f37d-405c-bc99-96ceec7982f5",
   "metadata": {},
   "source": [
    "* And finally ```db2```, which is the sum of the columns of ```dlogits```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63bfffb4-2b57-45ec-abe7-5ff9c2f2ac5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "db2             | exact True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "db2 = dlogits.sum(0, keepdim=True)\n",
    "cmp('db2', db2, b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfb0e18-4030-4477-9a61-88c0814cf494",
   "metadata": {},
   "source": [
    "* Next up is $dh/dhpreact$, which backpropagates through tanh\n",
    "* One form of the derivative of $tanh$ is $1/cosh^2(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "27c0c974-4e96-4921-bce8-fc39952d6833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dhpreact        | exact False | approximate: True  | maxdiff: 4.656612873077393e-10\n"
     ]
    }
   ],
   "source": [
    "dhpreact = (1. - h*h) * dh\n",
    "cmp('dhpreact', dhpreact, hpreact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91c6919a-e832-432d-9e05-a72ddd558ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 64]), torch.Size([32, 64]), torch.Size([1, 64]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bngain.shape, bnraw.shape, bnbias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b242d853-0f0f-4bed-99eb-be4429597d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbngain         | exact False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
      "dbnraw          | exact False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
      "dbnbias         | exact False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
      "dbndiff         | exact False | approximate: True  | maxdiff: 6.984919309616089e-10\n",
      "dbnvar_inv      | exact False | approximate: True  | maxdiff: 2.7939677238464355e-09\n",
      "dbndiff2        | exact False | approximate: True  | maxdiff: 2.9103830456733704e-11\n",
      "dbnmeani        | exact False | approximate: True  | maxdiff: 2.7939677238464355e-09\n",
      "dhprebn         | exact False | approximate: True  | maxdiff: 6.984919309616089e-10\n",
      "dembcat         | exact False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
      "dW1             | exact False | approximate: True  | maxdiff: 4.540197551250458e-09\n",
      "db1             | exact False | approximate: True  | maxdiff: 2.561137080192566e-09\n",
      "dC              | exact False | approximate: True  | maxdiff: 5.587935447692871e-09\n"
     ]
    }
   ],
   "source": [
    "dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "dbnraw = (bngain * dhpreact)\n",
    "dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "dbndiff = bnvar_inv * dbnraw\n",
    "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "dbnvar = -0.5 * (bnvar + 1e-5)**-1.5 * dbnvar_inv\n",
    "dbndiff2 = 1/(n-1) * torch.ones_like(bndiff2) * dbnvar\n",
    "dbndiff += 2*bndiff * dbndiff2\n",
    "dbnmeani = -dbndiff.sum(0, keepdim=True)\n",
    "dhprebn = (1/hprebn.shape[0])*torch.ones_like(hprebn)*dbnmeani\n",
    "dhprebn += dbndiff\n",
    "dembcat = dhprebn @ W1.T\n",
    "dW1 = embcat.T @ dhprebn\n",
    "db1 = dhprebn.sum(0, keepdim=True)\n",
    "demb = dembcat.view(-1, block_size, n_embd)\n",
    "dC =  torch.zeros_like(C)\n",
    "for k in range(Xb.shape[0]):\n",
    "    for j in range(Xb.shape[1]):\n",
    "        ix = Xb[k,j]\n",
    "        dC[ix] += demb[k,j]\n",
    "\n",
    "\n",
    "cmp('dbngain', dbngain, bngain)\n",
    "cmp('dbnraw', dbnraw, bnraw)\n",
    "cmp('dbnbias', dbnbias, bnbias)\n",
    "cmp('dbndiff', dbndiff, bndiff)\n",
    "cmp('dbnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "cmp('dbndiff2', dbndiff2, bndiff2)\n",
    "cmp('dbnmeani', dbnmeani, bnmeani)\n",
    "cmp('dhprebn', dhprebn, hprebn)\n",
    "cmp('dembcat', dembcat, embcat)\n",
    "cmp('dW1', dW1, W1)\n",
    "cmp('db1', db1, b1)\n",
    "cmp('dC', dC, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11520c96-b435-4d90-9e7d-ee5c25954c3b",
   "metadata": {},
   "source": [
    "## Excercise 2: Optimize Cross Entropy Loss Backward Pass\n",
    "* The Cross Entropy, i.e. the function that calculates the loss from the raw logits, is just a single call in torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a56ee8a9-e481-4550-90a3-75a0ad508a17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3698, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(logits, Yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f56dec9-c2c1-471f-a9fe-41661db8416d",
   "metadata": {},
   "source": [
    "* As it is a single function, the forward as well as the backward pass is much faster\n",
    "* If you write down the cross entropy function and it's derivative analytically, then you arrive at a simple function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be0593d-32b1-469d-b5a0-715157d79354",
   "metadata": {},
   "source": [
    "### Derivation of the Cross Entropy Loss Function\n",
    "\n",
    "If $\\Large P$ is the probability vector for one sample, then the loss for that sample is\n",
    "\n",
    "$\\Large loss = - log P_y$,\n",
    "\n",
    "where $\\Large P_y$ is the probability at the position that corresponds to the label $y$. The probability for an entry is caculated by the *softmax* function:\n",
    "\n",
    "$\\Large P_i=\\frac{e^{l_i}}{\\sum_j e^{l_j}}$\n",
    "\n",
    "So we want to calculate the derivative of the loss wrt to the logits:\n",
    "\n",
    "$\\Large \\frac{\\partial loss}{\\partial l_i}= \\frac{\\partial}{\\partial l_i}[-log \\frac{e^{l_y}}{\\sum_j e^{l_j}}  ]$\n",
    "\n",
    "#### Case: i=y\n",
    "\n",
    "First, we apply the chain rule for the outer $log$ function. Since the derivative of $log$ is $\\Large \\frac{1}{x}$\n",
    "\n",
    "$\\Large \\frac{\\partial loss}{\\partial l_y}= -\\frac{\\sum_j e^{l_j}}{e^{l_y}} \\cdot \\frac{\\partial}{\\partial l_i} [\\frac{e^{l_y}}{\\sum_j e^{l_j}}]$\n",
    "\n",
    "For the inner term, we apply the product rule for the division and again the chain rule for the second sum:\n",
    "\n",
    "$\\Large = -\\frac{\\sum_j e^{l_j}}{e^{l_y}} \\cdot (e^{l_y} \\cdot (\\sum_j e^{l_j})^{-1} - e^{l_y} \\cdot (\\sum_j e^{l_j})^{-2} \\cdot e^{l_y})$\n",
    "\n",
    "We can factor out $\\Large e^{l_y}$ and $\\Large (\\sum_j e^{l_j})^{-1}$, which cancel with the outer term. The minus sign is multiplied with both terms:\n",
    "\n",
    "$\\Large \\frac{\\cancel{\\sum_j e^{l_j}}}{\\cancel{e^{l_y}}} \\cdot (\\frac{- \\cancel{e^{l_y}} \\cdot \\sum_j e^{l_j} + e^{l_i} \\cdot \\cancel{e^{l_y}}}{(\\sum_j e^{l_j})^{\\cancel{2}}})$\n",
    "\n",
    "The term that is left can be summarized as follows:\n",
    "\n",
    "$\\Large P_y - 1$\n",
    "\n",
    "#### Case: i$\\neq$y\n",
    "\n",
    "Again, we apply the chain rule for the outer $log$ function.\n",
    "\n",
    "$\\Large \\frac{\\partial loss}{\\partial l_i}= -\\frac{\\sum_j e^{l_j}}{e^{l_y}} \\cdot \\frac{\\partial}{\\partial l_i} [\\frac{e^{l_y}}{\\sum_j e^{l_j}}]$\n",
    "\n",
    "As $e^{l_y}$ behaves like a constant this time, we need to apply the chain rule\n",
    "\n",
    "$\\Large = -\\frac{\\sum_j e^{l_j}}{e^{l_y}} \\cdot (-e^{l_y} \\cdot (\\sum_j e^{l_j})^{-2} \\cdot e^{l_i})$\n",
    "\n",
    "As with the last case, some terms cancel\n",
    "\n",
    "$\\Large = -\\frac{\\cancel{\\sum_j e^{l_j}}}{\\cancel{e^{l_y}}} \\cdot (-\\cancel{e^{l_y}} \\cdot (\\sum_j e^{l_j})^{-1} \\cdot e^{l_i})$\n",
    "\n",
    "and we are left with the probability for the logit $l_i$\n",
    "\n",
    "$\\Large = P_i$\n",
    "\n",
    "#### Divide by n\n",
    "\n",
    "In the two cases aboe, the loss for one element was derived wrt. $l_i$. The loss for all elements will add all the losses and take the average, so divide by $1/n$. The derivatives thus have to be divided by $1/n$ as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b6bf91-0305-4fd0-accf-aa4801e598ae",
   "metadata": {},
   "source": [
    "### In Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "04736c39-7a27-4d51-8f33-4c65fe0ed8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dlogits         | exact False | approximate: True  | maxdiff: 5.820766091346741e-09\n"
     ]
    }
   ],
   "source": [
    "dlogits = F.softmax(logits, 1)\n",
    "dlogits[range(n), Yb] -= 1\n",
    "dlogits /=  n\n",
    "\n",
    "cmp('dlogits', dlogits, logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2125cce-6013-4bd4-bf73-5052653b0126",
   "metadata": {},
   "source": [
    "### Intuitive Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "00b3f9f9-13c6-47cb-ba89-a7ade78089f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f85b03f9720>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(dlogits.detach(), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b792a8e-3874-45a1-81be-fe8797796e3e",
   "metadata": {},
   "source": [
    "* dlogits is the probability matrix of the forward pass, where the black dots in the image above are the entries where we subtracted a 1\n",
    "* Let's look at the first row of the probabilities and dlogits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a7c8b03f-603c-4f23-bd30-b1896a117409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0358, 0.0539, 0.0206, 0.0261, 0.0115, 0.0592, 0.0626, 0.0289, 0.0540,\n",
       "        0.0257, 0.0519, 0.0321, 0.0456, 0.0119, 0.0195, 0.0170, 0.0154, 0.0317,\n",
       "        0.0280, 0.0722, 0.0218, 0.0812, 0.0194, 0.0487, 0.0730, 0.0326, 0.0200],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(logits, 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f1ce4e4b-fe1c-41f1-b2dd-7b2963de10b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0358,  0.0539,  0.0206,  0.0261,  0.0115, -0.9408,  0.0626,  0.0289,\n",
       "         0.0540,  0.0257,  0.0519,  0.0321,  0.0456,  0.0119,  0.0195,  0.0170,\n",
       "         0.0154,  0.0317,  0.0280,  0.0722,  0.0218,  0.0812,  0.0194,  0.0487,\n",
       "         0.0730,  0.0326,  0.0200], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits[0] * n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a9c818-1d44-491e-9120-b9ba1bfbbd08",
   "metadata": {},
   "source": [
    "* We see that ```dlogits``` is exactly equal to the probabilities, except for the correct probability according to the label, where we subtracted 1\n",
    "* Notice that if we take the first row of ```dlogits``` and sum it, it will be equal to 0, because probabilities sum to 1 and we subtract 1 from one position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b60dd584-562e-407d-b42c-53ebc6e00b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-3.0268e-09, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits[0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aef5e2-16f8-4ffa-8a9c-c543f5bf0894",
   "metadata": {},
   "source": [
    "* Thus, we can think of the rows of pulling down on the probabilities of the incorrect characters and pulling up on the correct one\n",
    "* These pushes and pulls sum to zero, so the repulsion and the retraction are equal\n",
    "* the rest of the neural net, specifically the weights and biases give in to this tuck, as everything is mathematically connected\n",
    "* And the amount of push and pull is proportional to the probabilities in the forward pass, so if an incorrect answer got a high probability, it will be pushed down by that amount\n",
    "* In turn, if the correct character got a high probability, e.g. near 1, then subtracting by 1 leaves a small correction force"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357e331d-a8d0-4248-bc6d-a13810f92fbb",
   "metadata": {},
   "source": [
    "\n",
    "## Excercie 3: Optimize Batchnorm Backward Pass\n",
    "* Like with the cross entropy layer, we will derive the batch normalization formula wrt. the output of the linear layer\n",
    "* The formula for the batch normalization layer is as follows\n",
    "\n",
    "  $\\Large \\mu_B = \\frac{1}{m} \\sum_i^m x_i $\n",
    "  \n",
    "  $\\Large \\sigma_B^2 = \\frac{1}{m-1} \\sum_i^m (x_i - \\mu_B)^2$ // bessel correction\n",
    "\n",
    "  $\\Large \\hat{x}_i = \\frac{(x_i - \\mu_B)}{\\sqrt{\\sigma_B^2 + \\epsilon}}$\n",
    "\n",
    "  $\\Large y_i = \\gamma \\cdot \\hat{x}_i + \\beta$\n",
    "\n",
    "* We will compute the following derivative\n",
    "\n",
    "  $\\Large \\frac{\\partial loss}{\\partial x_i}$\n",
    "\n",
    "* and we already have\n",
    "\n",
    "  $\\Large \\frac{\\partial loss}{\\partial y_i}$\n",
    "\n",
    "* We will derive in the following order:\n",
    "\n",
    "  $\\Large \\frac{\\partial loss}{\\partial y_i}\\frac{\\partial y_i}{\\partial \\hat{x}_i},\n",
    "          \\frac{\\partial loss}{\\partial \\hat{x}_i}\\frac{\\partial \\hat{x}_i}{\\partial \\sigma_B^2},\n",
    "          \\frac{\\partial loss}{\\partial \\hat{x}_i}\\frac{\\partial \\hat{x}_i}{\\partial \\mu_B},\n",
    "          \\frac{\\partial loss}{\\partial \\sigma_B^2}\\frac{\\partial \\sigma_B^2}{\\partial \\mu_B},\n",
    "          \\frac{\\partial loss}{\\partial \\hat{x}_i}\\frac{\\partial \\hat{x}_i}{\\partial x_i},\n",
    "          \\frac{\\partial loss}{\\partial \\sigma_B^2}\\frac{\\partial \\sigma_B^2}{\\partial x_i},\n",
    "          \\frac{\\partial loss}{\\partial \\mu_B}\\frac{\\partial \\mu_B}{\\partial x_i}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26f755c-a8bb-453e-b388-9a1bf0a3bf30",
   "metadata": {},
   "source": [
    "* Here are the derivatives\n",
    "\n",
    "  $\\Large \\frac{\\partial loss}{\\partial \\hat{x}_i} = \\frac{\\partial loss}{\\partial y_i} \\frac{\\partial}{\\partial \\hat{x}_i} [ \\gamma \\hat{x}_i + \\beta ] = \\frac{\\partial loss}{\\partial y_i} \\gamma $\n",
    "  \n",
    "  $\\Large \\frac{\\partial loss}{\\partial \\sigma_B^2} = \\sum_i \\frac{\\partial loss}{\\partial \\hat{x}_i}\\frac{\\partial }{\\partial \\sigma_i^2}[\\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}] = -\\frac{1}{2}\\gamma \\sum_i \\frac{\\partial loss}{\\partial y_i} (x_i-\\mu_B) (\\sigma_B^2 + \\epsilon)^{-\\frac{3}{2}}$\n",
    "\n",
    "  $\\Large \\frac{\\partial loss}{\\partial \\mu_B} = (\\sum_i \\frac{\\partial loss}{\\partial \\hat{x}_i} \\frac{\\partial \\hat{x}_i}{\\mu_B}) + \\frac{\\partial loss}{\\partial \\sigma_B^2}\\frac{\\partial \\sigma_B^2}{\\partial \\mu_B}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eb54ec-a9df-45aa-b0d8-9aacc48270f1",
   "metadata": {},
   "source": [
    "* We'll derive the two addends separately, starting with the first\n",
    "\n",
    "  $\\Large \\frac{\\partial }{\\partial \\mu_B}[\\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}] = -(\\sigma_B^2 + \\epsilon)^{-\\frac{1}{2}}$\n",
    "\n",
    "* Then the second\n",
    "\n",
    "  $\n",
    "  \\begin{align}\n",
    "  \\frac{\\partial}{\\partial \\mu_B}[\\frac{1}{m}\\sum_i (x_i - \\mu_B)^2 ] &= -\\frac{2}{m-1}\\sum_i^m (x_i - \\mu_B) \\\\\n",
    "   &= -\\frac{2}{m-1} (\\sum_i^m x_i - \\sum_i^m \\mu_B) \\\\\n",
    "   &= -\\frac{2}{m-1} (\\sum_i^m x_i - m \\mu_B) \\\\\n",
    "   &= -\\frac{2m}{m-1} (\\frac{1}{m} \\sum_i^m x_i - \\mu_B)) \\\\\n",
    "   &= -\\frac{2m}{m-1} (\\mu_B - \\mu_B)) \\\\\n",
    "   &= 0\n",
    "  \\end{align}\n",
    "   $\n",
    "* Using our results in the original derivation\n",
    "\n",
    "  $\\Large \\frac{\\partial loss}{\\partial \\mu_B} = -\\sum_i \\frac{\\partial loss}{\\partial y_i}\\gamma (\\sigma_B^2 + \\epsilon)^{-\\frac{1}{2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6c5dbb-de2a-4115-9daa-e348d446577c",
   "metadata": {},
   "source": [
    "* Now on to the largest derivation: $x_i$\n",
    "\n",
    "  $\\Large \\frac{\\partial loss}{\\partial x_i} = \\frac{\\partial loss}{\\partial \\hat{x}_i} \\frac{\\partial \\hat{x}_i}{\\partial x_i} + \\frac{\\partial loss}{\\partial \\sigma_B^2} \\frac{\\partial \\sigma_B^2}{\\partial x_i} + \\frac{\\partial loss}{\\partial \\mu_B} \\frac{\\partial \\mu_B}{\\partial \\partial x_i}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a3b34b-33c7-4e54-a6e1-e591390f1a52",
   "metadata": {},
   "source": [
    "* Again, let's break it down per addend\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\hat{x}_i}{\\partial x_i} &= \\frac{\\partial}{\\partial x_i}[\\frac{(x_i - \\mu_B)}{\\sqrt{\\sigma_B^2 + \\epsilon}}] \\\\\n",
    "                                        &= (\\sigma_B^2 + \\epsilon)^{-\\frac{1}{2}}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\sigma_B^2}{\\partial x_i} &= \\frac{\\partial}{\\partial x_i}[\\frac{1}{m-1}\\sum_j^m (x_j - \\mu_B)^2] \\\\\n",
    "                                         &= \\frac{2}{m-1}(x_i - \\mu_B)\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mu_B}{\\partial x_i} &= \\frac{\\partial}{\\partial x_i}[\\frac{1}{m}\\sum_j^m x_i] \\\\\n",
    "                                    &= \\frac{1}{m}\n",
    "\\end{align}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f26ebd-f376-421a-a41d-20befb73121e",
   "metadata": {},
   "source": [
    "* We insert our results back into the large formula\n",
    "\n",
    "  $\\Large \\frac{\\partial loss}{\\partial x_i} = \\frac{\\partial loss}{\\partial \\hat{x}_i} (\\sigma_B^2 + \\epsilon)^{-\\frac{1}{2}} + \\frac{\\partial loss}{\\partial \\sigma_B^2} (\\frac{2}{m-1}(x_i - \\mu_B)) + \\frac{\\partial loss}{\\partial \\mu_B} \\frac{1}{m}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c6b834-7abd-4c05-ab56-3fa426a8a4c5",
   "metadata": {},
   "source": [
    "* And we expand the results that we achieved before\n",
    "\n",
    "  $\\Large \\frac{\\partial loss}{\\partial x_i} = \\frac{\\partial loss}{\\partial y_i}\\gamma (\\sigma_B^2 + \\epsilon)^{-\\frac{1}{2}} + -\\frac{1}{2}\\gamma (\\sum_j \\frac{\\partial loss}{\\partial y_j} (x_j-\\mu_B) (\\sigma_B^2 + \\epsilon)^{-\\frac{3}{2}}) (\\frac{2}{m-1}(x_i - \\mu_B)) -\\sum_j \\frac{\\partial loss}{\\partial y_j}\\gamma (\\sigma_B^2 + \\epsilon)^{-\\frac{1}{2}} \\frac{1}{m}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07343aad-d490-4150-ac21-95eaa661ad79",
   "metadata": {},
   "source": [
    "* We will now simplify with the goal to reuse calculations that we already computed\n",
    "* Every term has $(\\sigma_B^2 + \\epsilon)^{-\\frac{1}{2}}$\n",
    "* I don't know why, but Andrej is also factoring out $\\frac{1}{m}$\n",
    "* In the middle term, the $(\\sigma_B^2 + \\epsilon)^{-\\frac{3}{2}}=\\frac{1}{\\sqrt{(\\sigma_B^2 + \\epsilon)}^3}$ is factored into three $\\frac{1}{\\sqrt{(\\sigma_B^2 + \\epsilon)}}$\n",
    "* One $\\frac{1}{\\sqrt{(\\sigma_B^2 + \\epsilon)}}$ is factored out of the whole sum, one remains where it is and one is moved to the neighboring term to be able to reduce it to $\\hat{x}_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2eafbe8-5359-441d-9454-4e83d067f3a4",
   "metadata": {},
   "source": [
    "  $\\Large = \\frac{\\partial loss}{\\partial y_i}\\gamma (\\sigma_B^2 + \\epsilon)^{-\\frac{1}{2}} + -\\frac{1}{\\cancel{2}\\sqrt{(\\sigma_B^2 + \\epsilon)}}\\gamma \\sum_j \\frac{\\partial loss}{\\partial y_j} \\frac{(x_j-\\mu_B)}{\\sqrt{(\\sigma_B^2 + \\epsilon)}} (\\frac{\\cancel{2}}{m-1}\\frac{(x_j-\\mu_B)}{\\sqrt{(\\sigma_B^2 + \\epsilon)}}) -\\sum_j \\frac{\\partial loss}{\\partial y_j}\\gamma (\\sigma_B^2 + \\epsilon)^{-\\frac{1}{2}}\\frac{1}{m}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760a63f1-7062-4476-938f-6c055712659b",
   "metadata": {},
   "source": [
    "  $\\Large = \\frac{(\\sigma_B^2 + \\epsilon)^{-\\frac{1}{2}}}{m}\\gamma(\\frac{\\partial loss}{\\partial y_i} m - (\\frac{m}{m-1}\\hat{x}_i)(\\sum_j \\frac{\\partial loss}{\\partial y_j} \\hat{x}_j)-\\sum_j \\frac{\\partial loss}{\\partial y_j})$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7efa7a4-ffdc-4b4f-b63b-02dbff4b9da7",
   "metadata": {},
   "source": [
    "### In Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ccbdcfbb-f6e1-40d5-9c3b-6f963552ccdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dhprebn = bngain*bnvar_inv/n * (n*dhpreact - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0) - dhpreact.sum(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e8b1b38a-d202-446a-b0d5-4a87e38d0e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dhprebn         | exact False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
     ]
    }
   ],
   "source": [
    "cmp('dhprebn', dhprebn, hprebn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba81684-f29d-4e8e-ae5f-2059474edc8f",
   "metadata": {},
   "source": [
    "## Excercise 4: Put it all together!\n",
    "* We will now train the neural net using our own backprop code and compare to the result that torch give us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7462b608-7b5e-4b0e-9e84-63f54ca6df51",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size, n_embd)               , generator = g)\n",
    "W1 = torch.randn((block_size*n_embd, n_hidden)     , generator = g) * 5/3 / ((block_size*n_embd)**0.5) # kaimin initialization to avoid contraction\n",
    "b1 = torch.randn((n_hidden)                        , generator = g) * 0.1\n",
    "W2 = torch.randn((n_hidden, vocab_size)            , generator = g) * 0.1  # make less confident\n",
    "b2 = torch.randn((vocab_size)                      , generator = g) * 0.1  # not zero to unmask gradient errors\n",
    "\n",
    "bngain = torch.ones((1, n_hidden)) * 0.1 + 1.0\n",
    "bnbias = torch.zeros((1, n_hidden)) * 0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16be3ad-6392-458b-b10b-66c3b0737c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "n = batch_size # shorter name for use in expressions\n",
    "lossi = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce895e2-b8f4-4752-8072-b0385c648f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(max_steps):\n",
    "    xi = torch.randint(0, len(words), (batch_size, ), generator = g)\n",
    "    Xb, Yb = Xtr[xi], Ytr[xi]\n",
    "    # forward pass \"chunkated\" into smaller steps that are possible to backward one at a time\n",
    "    emb = C[Xb]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    # Linear Layer 1\n",
    "    hprebn = embcat @ W1 + b1\n",
    "    # Batchnorm Layer\n",
    "    bnmean = hprebn.mean(0, keepdim=True)\n",
    "    bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
    "    bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "    bnraw = (hprebn - bnmean) * bnvar_inv\n",
    "    hpreact = bngain * bnraw + bnbias\n",
    "    # Non Linearity\n",
    "    h = torch.tanh(hpreact)\n",
    "    # Linear layer 2\n",
    "    logits = h @ W2 + b2\n",
    "    # cross entropy loss\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "    \n",
    "    # PyTorch backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    dlogits = F.softmax(logits, 1)\n",
    "    dlogits[range(n), Yb] -= 1\n",
    "    dlogits /=  n\n",
    "    dh = dlogits @ W2.T\n",
    "    dW2 = h.T @ dlogits\n",
    "    db2 = dlogits.sum(0)\n",
    "    dhpreact = (1. - h*h) * dh\n",
    "    dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "    dbnraw = (bngain * dhpreact)\n",
    "    dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "    dbndiff = bnvar_inv * dbnraw\n",
    "    dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "    dbnvar = -0.5 * (bnvar + 1e-5)**-1.5 * dbnvar_inv\n",
    "    dbndiff2 = 1/(n-1) * torch.ones_like(bndiff2) * dbnvar\n",
    "    dbndiff += 2*bndiff * dbndiff2\n",
    "    dbnmeani = -dbndiff.sum(0, keepdim=True)\n",
    "    dhprebn = (1/hprebn.shape[0])*torch.ones_like(hprebn)*dbnmeani\n",
    "    dhprebn += dbndiff\n",
    "    dembcat = dhprebn @ W1.T\n",
    "    dW1 = embcat.T @ dhprebn\n",
    "    db1 = dhprebn.sum(0)\n",
    "    demb = dembcat.view(-1, block_size, n_embd)\n",
    "    dC =  torch.zeros_like(C)\n",
    "    for k in range(Xb.shape[0]):\n",
    "        for j in range(Xb.shape[1]):\n",
    "            ix = Xb[k,j]\n",
    "            dC[ix] += demb[k,j]\n",
    "\n",
    "    grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
    "\n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01\n",
    "    for p, grad in zip(parameters, grads):\n",
    "#        p.data += -lr * p.grad\n",
    "        p.data += -lr * grad\n",
    "\n",
    "    # track stats \n",
    "    if i % 10000 == 0:\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac49f11-9524-4e72-9bd3-05f1dd84d9e3",
   "metadata": {},
   "source": [
    "* After training, we calculate the mean and var for the whole training set to be used in the forward pass that calculates the loss of a whole set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e4ad1f-1d49-4b0b-9ec7-c24a5847075f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    emb = C[Xtr]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    bnmean = hpreact.mean(0, keepdim=True)\n",
    "    bnvar = hpreact.var(0, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4de810-f6f2-4d4a-9bd1-aac005cc526b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def split_loss(split):\n",
    "    x, y = {\n",
    "        'train': (Xtr, Ytr),\n",
    "        'val': (Xval, Yval),\n",
    "        'test': (Xtest, Ytest),\n",
    "    }[split]\n",
    "    emb = C[x]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1810b4-a598-4160-bc76-b816d9e0c4e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcf53e2-178c-4704-959f-a83b57509e2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30750989-b2c7-4cee-a1e1-d7e5c0df0e96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a3e910-412b-4807-bc57-972c1c9bc604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000f85ed-f2dd-4b19-a71e-c57091525739",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59eb7518-f1b2-4ce7-8138-73a22a320346",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51099d8-9079-40ac-b49e-00b01d2fb979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fb81d9-1163-4f49-b642-00d9414bc773",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e577a1de-8c78-49c4-9907-d81c4cf9b8fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebf56b0-5ea8-49ea-8312-1d0c13a3b422",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ac4be8-d3da-4e76-8d79-d08504afc858",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe84073-bca7-4ade-815e-1026605cbba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253e288b-2e8a-41e2-a166-8a6051f07494",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7adb80b-2feb-41c3-8f9c-93ccf470c872",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9399feb-fc80-4e14-bf11-077ae8b17065",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
