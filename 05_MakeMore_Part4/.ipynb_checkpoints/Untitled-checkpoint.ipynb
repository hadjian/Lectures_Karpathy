{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2673859b-4d8a-4881-8fa3-11860cdb9878",
   "metadata": {},
   "source": [
    "# Becoming a Backprop Ninja\n",
    "* You need to understand backpropagation, because it is a leaky abstraction\n",
    "* We already covered backpropagation for the scalar case, by implementing micrograd\n",
    "* But we need to expand this knowledge to tensors\n",
    "* We will use the same neural network as in the last lecture, but this time we will implement the backward pass manually"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78c9a6e-1466-4519-965d-f6a239dc480a",
   "metadata": {},
   "source": [
    "## The Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c92b2632-e797-4c34-9a8d-86a0a01468ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfa6c142-c68f-4995-aecf-ea2824a66197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n",
      "15\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
     ]
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "print(len(words))\n",
    "print(max(len(w) for w in words))\n",
    "print(words[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09cdcbfc-6da8-4460-b353-8b43331b4f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = list(sorted(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "vocab_size = len(stoi)\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75291b4a-8782-4c1e-a0ba-e6596ea2c272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dataset\n",
    "block_size = 3\n",
    "def build_dataset(words):\n",
    "    X,Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * 3\n",
    "        for c in w+'.':\n",
    "            xi = stoi[c]\n",
    "            X.append(context)\n",
    "            Y.append(xi)\n",
    "            context = context[1:]+[xi]\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X,Y\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.8*len(words))\n",
    "Xtr,Ytr=build_dataset(words[0:n1])\n",
    "Xval,Yval=build_dataset(words[n1:n2])\n",
    "Xtest, Ytest=build_dataset(words[n2:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef86fbd-2aa0-4708-b27b-ba5e776d2f23",
   "metadata": {},
   "source": [
    "* A new utility function is introduced that compares our manual gradient computations with pytorch computed ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "895ee718-104d-42dd-9e12-02731e89480c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmp(s, dt, t):\n",
    "    ex = torch.all(dt == t.grad).item()\n",
    "    app = torch.allclose(dt, t.grad)\n",
    "    maxdiff = (dt - t.grad).abs().max().item()\n",
    "    print(f'{s:15s} | exact {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4de40b8-5106-4928-89fb-432186628c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size, n_embd)               , generator = g)\n",
    "W1 = torch.randn((block_size*n_embd, n_hidden)     , generator = g) * 5/3 / ((block_size*n_embd)**0.5) # kaimin initialization to avoid contraction\n",
    "b1 = torch.randn((n_hidden)                        , generator = g) * 0.1\n",
    "W2 = torch.randn((n_hidden, vocab_size)            , generator = g) * 0.1  # make less confident\n",
    "b2 = torch.randn((vocab_size)                      , generator = g) * 0.1  # not zero to unmask gradient errors\n",
    "\n",
    "bngain = torch.ones((1, n_hidden)) * 0.1 + 1.0\n",
    "bnbias = torch.zeros((1, n_hidden)) * 0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd31673-2a92-4953-87d9-0acb479d650e",
   "metadata": {},
   "source": [
    "* We will do a single forward pass and for this calculate one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a347a472-e6c2-4097-b57c-753009b1594c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size # shorter name for use in expressions\n",
    "xi = torch.randint(0, len(words), (batch_size, ), generator = g)\n",
    "Xb, Yb = Xtr[xi], Ytr[xi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ace2fe6a-deed-47a8-9797-bcc756edd68d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.8712, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass \"chunkated\" into smaller steps that are possible to backward one at a time\n",
    "emb = C[Xb]\n",
    "embcat = emb.view(-1, block_size*n_embd)\n",
    "# Linear Layer 1\n",
    "hprebn = embcat @ W1 + b1\n",
    "# Batchnorm Layer\n",
    "bnmeani = hprebn.mean(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1/(n-1) * bndiff2.sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1 instead of n)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "# Non Linearity\n",
    "h = torch.tanh(hpreact)\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2\n",
    "# cross entropy loss\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# PyTorch backward pass\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv,\n",
    "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
    "          bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
    "          embcat, emb]:\n",
    "    t.retain_grad()\n",
    "loss.backward()\n",
    "loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d09fbc-f949-4cee-b553-caeeaf95103a",
   "metadata": {},
   "source": [
    "* Exercise 1: backprop through the whole thing manually, backpropagating through exactly all of the variables as they are defined in the forward pass above, one by one\n",
    "* We start with dlogprops, which has the following shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9527d14-d38d-4f53-9c4c-50a6c1991b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprobs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3264bd3-ece9-46a5-ba77-44d0780af050",
   "metadata": {},
   "source": [
    "* The gradient tensor must have the same shape as we need the element-wise gradient\n",
    "* So how does ```logprobs``` influence ```loss```?\n",
    "* ```loss``` is a result of an index operation into ```logprobs``` and a mean calculation of all the resulting values\n",
    "* The result is then negated\n",
    "* The indices for each row of ```logprobs``` is taken from the row vector ```Yb```, which are all the correct labels for the 32 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4d29438-71c5-42a3-88d1-f37d47f5c7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0, 15,  1,  9, 18, 20,  5, 14,  0, 25,  5,  9,  9,  9, 20,  0,  0,  9,\n",
      "         1,  2,  1,  0,  9,  1,  1,  1,  9, 14,  3, 12, 14,  5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(Yb)\n",
    "Yb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec0a41a-2c09-40b5-bf31-c3aaadad5d18",
   "metadata": {},
   "source": [
    "* So in a simpler example, where we have three indexes, the loss becomes\n",
    "  \n",
    "  $loss=-(a+b+c)/3=-a/3 - b/3 - c/3$\n",
    "* So deriving by each variable becomes for the example of $a$:\n",
    "  \n",
    "  $dloss/da=-1/3$\n",
    "\n",
    "* Or more generally $-1/n$ for $n$ variables\n",
    "* But only one number in the 32 rows is used in the $loss$ calculation, so the rest don't influence the $loss$ at all and thus receive a gradient of 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f36a46ce-3f98-4142-9467-c4a0286d073a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs[range(n), Yb] = -1.0/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "321f9ae5-f148-4614-bd25-61ccd2a7d4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "cmp('logprobs', dlogprobs, logprobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367617d7-0f14-432c-961a-38d482272f4f",
   "metadata": {},
   "source": [
    "* We continue with deriving ```logprobs``` by it's variables\n",
    "* ```logprobs``` takes as it's variable only ```probs``` and results from applying ```log``` to every entry\n",
    "* So each entry of ```probs``` changes ```logprobs``` by the proportionality factor of the derivative of ```log```, which is ```1/x```, where ```x``` is the entry\n",
    "* As ```logprobs``` and ```probs``` have the same dimensions, we just calculate ```1/x``` for each entry\n",
    "* Also we need to apply the chain rule, by multiplying by the gradients of logprobs to arrive at the derivatives of ```probs``` with respect to ```loss```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b865306d-aabd-42da-b2ed-4e53169d15c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dprobs = 1/probs * dlogprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d47ed42-2677-4186-af0d-48e06640eec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dprobs          | exact True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "cmp('dprobs', dprobs, probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc0465a-2b3c-4c60-aed5-190392c1afe3",
   "metadata": {},
   "source": [
    "* Moving on to the constituents of ```probs```, which is simply computed by multiplying the variables ```counts``` and ```counts_sum_inv```\n",
    "* Applying the derivative for multiplication (example $a$)\n",
    "  \n",
    "  $d(counts\\_sum\\_inv*counts)/dcounts\\_sum\\_inv=counts$\n",
    "* So for ```counts_sum_inv```, it will be a gradient of ```counts```\n",
    "* But we need to take into account that ```counts_sum_inv``` has a different dimensionality than ```probs```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f43f0bbc-fa4f-4d98-ad7e-5ad08570fef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]), torch.Size([32, 1]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape, counts_sum_inv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23b0b43-fa5a-4eef-b006-2c73646357ee",
   "metadata": {},
   "source": [
    "* We can see, that the one column of $counts_sum_inv$ will be broadcast into every of the 27 columns of ```probs```\n",
    "* We learned in the micrograd lecture that if a variable takes part in multiple expressions the gradients of those expression must be summed for that variable\n",
    "* One element of the ```counts_sum_inv``` column vector is used in 27 multiplications in one row of ```counts```, thus we will sum all the gradients in that row, which is the sum of ```counts```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d337564-8ee4-4d24-a846-c7f2053824fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dcounts_sum_inv | exact True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dcounts_sum_inv= (counts*dprobs).sum(1, keepdim=True)\n",
    "cmp('dcounts_sum_inv', dcounts_sum_inv, counts_sum_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4392ecc-1660-4178-bcb7-934f4c1a3304",
   "metadata": {},
   "source": [
    "* Next up is ```counts```, which appears in the two expressions that result in ```probs``` and ```counts_sum```\n",
    "* So before we can compute the derivative of ```counts```, we first need to derive ```counts_sum_inv``` wrt ```counts_sum```\n",
    "* ```counts_sum``` is inverted, so the derivative becomes\n",
    "\n",
    "  $d(counts\\_sum^{-1})/dcounts\\_sum=-counts\\_sum^{-2}$\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40da0065-7a16-487f-a56a-20ac3f3811a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dcounts_sum     | exact True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dcounts_sum = -1*counts_sum**-2 * dcounts_sum_inv\n",
    "cmp('dcounts_sum', dcounts_sum, counts_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c673a5-de8e-4cee-bcfe-eb22c411f637",
   "metadata": {},
   "source": [
    "* Now we can compute the derivative of ```counts```\n",
    "* As all variables of ```counts``` are summed row wise in ```counts_sum```, the derivative will be one for each variable\n",
    "\n",
    "  $d(counts\\_sum)/dcounts=1$\n",
    "\n",
    "* In ```probs```, ```counts``` is multiplied with the broadcasted ```counts_sum_inv```, so\n",
    "\n",
    "  $d(probs)/dcounts=d(counts * counts\\_sum\\_inv)/dcounts=counts\\_sum\\_inv$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92684425-fc1b-43a5-9c91-2da7836ea320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dcounts         | exact True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dcounts = torch.ones_like(counts) * dcounts_sum\n",
    "dcounts += counts_sum_inv * dprobs\n",
    "cmp('dcounts', dcounts, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87d054c0-a460-4522-b522-f7295ac9b306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dnorm_logits    | exact True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dnorm_logits = counts * dcounts\n",
    "cmp('dnorm_logits', dnorm_logits, norm_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3bc071e1-1b37-47f5-803c-dd8c5c2b3c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dlogit_maxes    | exact True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dlogit_maxes = (-dnorm_logits).sum(1, keepdim=True)\n",
    "cmp('dlogit_maxes', dlogit_maxes, logit_maxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08500a4a-bba0-4c5d-8286-95d11469d757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dlogits         | exact True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dlogits = torch.zeros_like(logits)\n",
    "dlogits[range(n), logits.max(1).indices] = 1\n",
    "dlogits *= dlogit_maxes\n",
    "dlogits += dnorm_logits.clone()\n",
    "cmp('dlogits', dlogits, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec4a263e-a0e3-4421-8880-6ddb29275ccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 200])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dW2 = torch.ones_like(W2)\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48eae1b6-87d1-4703-ab50-c69c966480e5",
   "metadata": {},
   "source": [
    "* Now we move on to ```logits = h @ W2 + b2```\n",
    "* Writing down the whole matrix multiply element by element shows that the partial derivatives of the expression ```h @ W2 + b2``` are also matrix multiplications, in particular\n",
    "\n",
    "  $dL/dh = dL/dlogits * W2^T$\n",
    "  \n",
    "  $dL/dW2 = h^T * dL/dlogits$\n",
    "\n",
    "* The offset b2 is broadcast across the columns of the result matrix, so each entry in a column of the $dL/dlogits$ matrix will add to the partial derivative of the $b1$ tensor\n",
    "  \n",
    "  $dL/db2 = dL/dlogits.sum(0)$\n",
    "\n",
    "* A shortcut to avoid having to remember these formulas is to look at the shapes of the operations, which have to match up\n",
    "* For example, the result of $dL/dh$ must match the dimensions of $h$\n",
    "* It results from a matrix/vector multiplication of the other factor in the product ($W2$ in this example) with $dL/dlogits$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6bf956e3-b1aa-456c-a4b1-0f6b6ea18167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: torch.Size([32, 200])\n",
      "W2: torch.Size([200, 27])\n",
      "dlogits: torch.Size([32, 27])\n"
     ]
    }
   ],
   "source": [
    "print(f'h: {h.shape}')\n",
    "print(f'W2: {W2.shape}')\n",
    "print(f'dlogits: {dlogits.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59f945e-7d99-4276-83d4-1082a6060b36",
   "metadata": {},
   "source": [
    "* The only way we arrive at a dimension of ```[32,200]``` is to multiply ```dlogits``` with ```W2``` transposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b7d03f88-e92a-43dd-8821-203fabb843a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h               | exact True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dh = dlogits @ W2.T\n",
    "cmp('h', dh, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d34931-93ef-48c2-8d07-4efd829a88f1",
   "metadata": {},
   "source": [
    "* Same for $dL/dW2$, which must have the same dimensions as ```W2``` and results from a matrix/vector multiplication of ```dlogits``` and ```h```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb088b75-75c5-433f-ac4a-8147bdb7242e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2: torch.Size([200, 27])\n",
      "h: torch.Size([32, 200])\n",
      "dlogits: torch.Size([32, 27])\n"
     ]
    }
   ],
   "source": [
    "print(f'W2: {W2.shape}')\n",
    "print(f'h: {h.shape}')\n",
    "print(f'dlogits: {dlogits.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d232b16e-2763-40d8-88fc-6b76d125fcae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW2             | exact True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dW2 = h.T @ dlogits\n",
    "cmp('dW2', dW2, W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796e19f8-f37d-405c-bc99-96ceec7982f5",
   "metadata": {},
   "source": [
    "* And finally ```db2```, which is the sum of the columns of ```dlogits```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "63bfffb4-2b57-45ec-abe7-5ff9c2f2ac5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "db2             | exact True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "db2 = dlogits.sum(0, keepdim=True)\n",
    "cmp('db2', db2, b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfb0e18-4030-4477-9a61-88c0814cf494",
   "metadata": {},
   "source": [
    "* Next up is $dh/dhpreact$, which backpropagates through tanh\n",
    "* One form of the derivative of $tanh$ is $1/cosh^2(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "27c0c974-4e96-4921-bce8-fc39952d6833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dhpreact        | exact False | approximate: True  | maxdiff: 4.656612873077393e-10\n"
     ]
    }
   ],
   "source": [
    "dhpreact = (1. - h*h) * dh\n",
    "cmp('dhpreact', dhpreact, hpreact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "91c6919a-e832-432d-9e05-a72ddd558ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 200]), torch.Size([32, 200]), torch.Size([1, 200]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bngain.shape, bnraw.shape, bnbias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b242d853-0f0f-4bed-99eb-be4429597d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbngain         | exact False | approximate: True  | maxdiff: 2.7939677238464355e-09\n",
      "dbnraw          | exact False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
      "dbnbias         | exact False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
      "dbndiff         | exact False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
      "dbnvar_inv      | exact False | approximate: True  | maxdiff: 3.026798367500305e-09\n",
      "dbndiff2        | exact False | approximate: True  | maxdiff: 2.9103830456733704e-11\n",
      "dbnmeani        | exact False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
      "dhprebn         | exact False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
      "dembcat         | exact False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
      "dW1             | exact False | approximate: True  | maxdiff: 5.122274160385132e-09\n",
      "db1             | exact False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
      "dC              | exact False | approximate: True  | maxdiff: 9.313225746154785e-09\n"
     ]
    }
   ],
   "source": [
    "dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "dbnraw = (bngain * dhpreact)\n",
    "dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "dbndiff = bnvar_inv * dbnraw\n",
    "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "dbnvar = -0.5 * (bnvar + 1e-5)**-1.5 * dbnvar_inv\n",
    "dbndiff2 = 1/(n-1) * torch.ones_like(bndiff2) * dbnvar\n",
    "dbndiff += 2*bndiff * dbndiff2\n",
    "dbnmeani = -dbndiff.sum(0, keepdim=True)\n",
    "dhprebn = (1/hprebn.shape[0])*torch.ones_like(hprebn)*dbnmeani\n",
    "dhprebn += dbndiff\n",
    "dembcat = dhprebn @ W1.T\n",
    "dW1 = embcat.T @ dhprebn\n",
    "db1 = dhprebn.sum(0, keepdim=True)\n",
    "demb = dembcat.view(-1, block_size, n_embd)\n",
    "dC =  torch.zeros_like(C)\n",
    "for k in range(Xb.shape[0]):\n",
    "    for j in range(Xb.shape[1]):\n",
    "        ix = Xb[k,j]\n",
    "        dC[ix] += demb[k,j]\n",
    "\n",
    "\n",
    "cmp('dbngain', dbngain, bngain)\n",
    "cmp('dbnraw', dbnraw, bnraw)\n",
    "cmp('dbnbias', dbnbias, bnbias)\n",
    "cmp('dbndiff', dbndiff, bndiff)\n",
    "cmp('dbnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "cmp('dbndiff2', dbndiff2, bndiff2)\n",
    "cmp('dbnmeani', dbnmeani, bnmeani)\n",
    "cmp('dhprebn', dhprebn, hprebn)\n",
    "cmp('dembcat', dembcat, embcat)\n",
    "cmp('dW1', dW1, W1)\n",
    "cmp('db1', db1, b1)\n",
    "cmp('dC', dC, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11520c96-b435-4d90-9e7d-ee5c25954c3b",
   "metadata": {},
   "source": [
    "## Excercise 2: Optimize Cross Entropy Loss Backward Pass\n",
    "* The Cross Entropy, i.e. the function that calculates the loss from the raw logits, is just a single call in torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a56ee8a9-e481-4550-90a3-75a0ad508a17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.8712, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(logits, Yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f56dec9-c2c1-471f-a9fe-41661db8416d",
   "metadata": {},
   "source": [
    "* As it is a single function, the forward as well as the backward pass is much faster\n",
    "* If you write down the cross entropy function and it's derivative analytically, then you arrive at a simple function\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d132905-23e4-4d8d-8c5c-ddbf2e6c3c01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded92e93-bd15-4957-9766-48f3e2895e80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04736c39-7a27-4d51-8f33-4c65fe0ed8c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190d4049-1b43-45a8-9db2-ec55f529e010",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70702efa-6015-4d9b-a6b2-2c8f38b501f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1dab4c-180f-4823-bbab-a23508131c91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f27e009-04a8-4a6c-85cd-a0b7ef379429",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ce5329-ab2f-499b-8070-909c1cd35287",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05946863-8804-4834-90be-bdd899ce0899",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b05ce46-4c79-4007-9986-798bb11ef741",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a35a2f-8509-444f-9a06-5701dc59ee4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b3f9f9-13c6-47cb-ba89-a7ade78089f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45901e1c-7d01-4014-bcaa-e16eaa3ac37f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c8b03f-603c-4f23-bd30-b1896a117409",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ce4e4b-fe1c-41f1-b2dd-7b2963de10b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f633ab6-d1a4-4994-a6d5-fec29583b02b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72295c08-431b-459f-a495-2319121361f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1415e0d6-4752-484a-8c42-29558c37a811",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7854b7-b041-49c8-896b-04229c3adf42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c747109-927e-449f-b50c-c35bd16a7cf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
